# Positional Encoding
## Overview:
**Where are we?**

![](images/positional_encoding/2025-04-16-19-37-59.png)
* Positional Encoding is responsible for representing the position of the word in the sentence

**Example:**

![](images/positional_encoding/2025-04-16-19-56-13.png)
* We do this by adding another vector (same size as the embedding) to encode the position of each word in the sentence.

<br>

## Equations
**Building Vectors:**

![](images/positional_encoding/2025-04-17-21-14-39.png)

<br>

## Code
* lines 17-43
![](images/positional_encoding/2025-04-18-01-05-13.png)

**Missing:**
* Equations Explaination

<br>

### Next up: [Layer Normalization](layer_normalization.md)
